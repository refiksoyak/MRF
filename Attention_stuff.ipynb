{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention stuff.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/refiksoyak/MRF/blob/master/Attention_stuff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQQKQQjLusog",
        "colab_type": "text"
      },
      "source": [
        "IN THIS NOTEBOOK, ATTENTION and ITS VARIENTS IMPLEMENTED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6TUHXXZtNla",
        "colab_type": "code",
        "outputId": "106ddfcf-1838-4240-8694-86fbf01b7af3",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import keras\n",
        "import h5py as h5\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "from sklearn.feature_extraction import image\n",
        "import numpy as np \n",
        "from skimage.util.shape import view_as_windows, view_as_blocks\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "import math\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmfuQv_osmT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/KCL internship/Task_1/Refik/Notebooks')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acL_uof6g_nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from deo import operations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuH9kqYiScT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "signal_length=2000\n",
        "skip_size=1\n",
        "\n",
        "t_type = 'T1' #T1, T2, T1_T2\n",
        "\n",
        "fold = 1\n",
        "\n",
        "if fold == 1:\n",
        "  test = 'kl' #kl, tk, ir, ab, abd\n",
        "  train1 = 'abd'\n",
        "  train2 = 'ir'\n",
        "\n",
        "elif fold == 2:\n",
        "  test = 'abd' \n",
        "  train1 = 'kl'\n",
        "  train2 = 'ir'\n",
        "\n",
        "elif fold==3:\n",
        "  test = 'ir'\n",
        "  train1 = 'abd'\n",
        "  train2 = 'kl'\n",
        "\n",
        "\n",
        "input_shape = 'pixel' #pixel, patch, image\n",
        "patch_size = 4\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcZFLhOsKt14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "signal_ab, T1_ab, T2_ab, signal_tk, T1_tk, T2_tk = operations.read_data('train', fold_number = fold)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86PuYgsF3Yez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLaS5_q5twvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "signal_ab = operations.handle_complexity(signal_ab)\n",
        "signal_tk = operations.handle_complexity(signal_tk)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57xwiwk3LZqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "signal_ab = operations.crop_image(signal_ab)\n",
        "T1_ab = operations.crop_image(T1_ab)\n",
        "T2_ab = operations.crop_image(T2_ab)\n",
        "\n",
        "signal_tk = operations.crop_image(signal_tk)\n",
        "T1_tk = operations.crop_image(T1_tk)\n",
        "T2_tk = operations.crop_image(T2_tk)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tyPyEMqUwG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ext_patch(signal,T1,T2,patch_size):\n",
        "\n",
        "  signal_patch=[]\n",
        "  T1_values = image.extract_patches_2d(T1, (patch_size, patch_size))\n",
        "  T2_values = image.extract_patches_2d(T2, (patch_size, patch_size))\n",
        "          \n",
        "  for i in range(signal.shape[2]):\n",
        "    c_signal=signal[:,:,i]\n",
        "    signal_patch.append(image.extract_patches_2d(c_signal, (patch_size, patch_size)))\n",
        "  return np.array(signal_patch), np.array(T1_values), np.array(T2_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC5Io5pgAeaM",
        "colab_type": "code",
        "outputId": "6e6d3972-39d3-468d-8c48-e1040b2b5083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "if input_shape=='pixel':\n",
        "  \n",
        "  signal = np.concatenate([signal_ab,signal_tk])\n",
        "  T1_values = np.concatenate([T1_ab,T1_tk])\n",
        "  T2_values = np.concatenate([T2_ab,T2_tk])\n",
        "  signal = signal.reshape(2*220*200,1,signal_length)\n",
        "  T1_values = T1_values.reshape(2*220*200,1,1)\n",
        "  T2_values = T2_values.reshape(2*220*200,1,1)\n",
        "  \n",
        "elif input_shape=='patch':\n",
        "  \n",
        "  #PATCH EXTRACTION\n",
        "  \"\"\"\n",
        "  signal_patch_ab = operations.patch_extracter(signal_ab, patch_size)\n",
        "  T1_patch_ab = operations.patch_extracter(T1_ab, patch_size)\n",
        "  T2_patch_ab = operations.patch_extracter(T2_ab, patch_size)\n",
        "  \"\"\"\n",
        "  signal_patch_ab, T1_patch_ab, T2_patch_ab = ext_patch(signal_ab, T1_ab, T2_ab, patch_size)\n",
        "  signal_patch_ab = np.moveaxis(signal_patch_ab, 0, -1)\n",
        "  #del signal_ab #delete signal so that is cause use of ram\n",
        "  \"\"\"\n",
        "  signal_patch_tk = operations.patch_extracter(signal_tk, patch_size)\n",
        "  T1_patch_tk = operations.patch_extracter(T1_tk, patch_size)\n",
        "  T2_patch_tk = operations.patch_extracter(T2_tk, patch_size)\n",
        "  \"\"\"\n",
        "  signal_patch_tk, T1_patch_tk, T2_patch_tk = ext_patch(signal_tk, T1_tk, T2_tk, patch_size)\n",
        "  signal_patch_tk = np.moveaxis(signal_patch_tk, 0, -1)\n",
        "  #del signal_tk #delete signal so that is cause use of ram\n",
        "\n",
        "  #concatenate of all patients' values before normalization\n",
        "  signal = np.concatenate((signal_patch_ab, signal_patch_tk)) #t1_tk was removed\n",
        "  signal = signal.reshape(-1, patch_size, patch_size, 2000)\n",
        "  del signal_patch_ab, signal_patch_tk\n",
        "  T1_values=np.concatenate((T1_patch_ab, T1_patch_tk))\n",
        "  T2_values=np.concatenate((T2_patch_ab, T2_patch_tk))\n",
        "  del T1_patch_ab, T1_patch_tk, T2_patch_ab, T2_patch_tk\n",
        "\n",
        "  T1_values = T1_values.reshape(-1, patch_size, patch_size, 1)\n",
        "  T2_values = T2_values.reshape(-1, patch_size, patch_size, 1)\n",
        "else:\n",
        "  print('Wrong input shape')\n",
        "  \n",
        "print('Signal shape:',signal.shape)\n",
        "print('T1_values shape:', T1_values.shape)\n",
        "print('T2_values shape:', T2_values.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Signal shape: (88000, 1, 2000)\n",
            "T1_values shape: (88000, 1, 1)\n",
            "T2_values shape: (88000, 1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmOj7aMitwzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#normalize data\n",
        "T1_values = operations.normalize(T1_values)\n",
        "T2_values = operations.normalize(T2_values)\n",
        "signal = operations.normalize(signal)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFytBnpNsW13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining output (target)\n",
        "if t_type == 'T1':\n",
        "  \n",
        "  T_val = T1_values\n",
        "  models_output_size = 1\n",
        "\n",
        "elif t_type == 'T2':\n",
        "\n",
        "  T_val = T2_values\n",
        "  models_output_size = 1\n",
        "\n",
        "elif t_type == 'T1_T2':\n",
        "  if input_shape == 'patch': axis = 3\n",
        "  elif input_shape == 'pixel': axis = 2\n",
        "  T_val = np.concatenate([T1_values,T2_values], axis = axis)\n",
        "  models_output_size = 2\n",
        "else:\n",
        "  print('Wrong t_type! Enter T1,T2 or T1_T2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am-1ShQcr515",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Models \n",
        "def dense():\n",
        "\n",
        "  inputs = keras.Input((patch_size, patch_size, 2000))\n",
        "  x = keras.layers.Dense(128,activation='relu',name='dense1')(inputs)\n",
        "  #x = keras.layers.BatchNormalization()(x)\n",
        "  x = keras.layers.Dense(64,activation='relu',name='dense2')(x)\n",
        "  #x = keras.layers.BatchNormalization()(x)\n",
        "  #x = keras.layers.Dense(32,activation='relu',name='dense3')(x)\n",
        "  #x = keras.layers.BatchNormalization()(x)\n",
        "  x = keras.layers.Dense(16,activation='relu',name='dense4')(x)\n",
        "  x = keras.layers.BatchNormalization()(x)\n",
        "  x = keras.layers.Dense(1, name='dense5')(x)\n",
        "\n",
        "  model = keras.models.Model(inputs = inputs, outputs = x)\n",
        "  return model\n",
        "\n",
        "#1D models\n",
        "\n",
        "def conv1D(spatial_attention = False, channel_attention = False):\n",
        "  inputs = keras.Input((1, 2000))\n",
        "  \n",
        "  x = keras.layers.Conv1D(32, 3, padding = 'same')(inputs)\n",
        "  x = keras.layers.Activation('relu')(x)\n",
        "  x = keras.layers.Conv1D(64, 3, padding = 'same')(x)\n",
        "  x = keras.layers.Activation('relu')(x)\n",
        "\n",
        "  if channel_attention:\n",
        "    x = channel_attn_1D(x, 64, 4 )\n",
        "\n",
        "  if spatial_attention:\n",
        "    x = spatial_attn_1D(x, 64) #Spatial attention\n",
        "\n",
        "  x = keras.layers.Conv1D(128, 3, padding = 'same')(x)\n",
        "  x = keras.layers.Activation('relu')(x)\n",
        "  x = keras.layers.Conv1D(64, 3, padding = 'same')(x)\n",
        "  x = keras.layers.Activation('relu')(x)\n",
        "  x = keras.layers.Dense(1, activation='relu')(x)\n",
        "\n",
        "  return keras.models.Model(inputs = inputs, outputs = x)\n",
        "\n",
        "\n",
        "def spatial_attn_1D(x, nb_filter):\n",
        "  \n",
        "  avg_pl = keras.layers.AveragePooling1D(pool_size=1)(x)\n",
        "  max_pl = keras.layers.MaxPooling1D(pool_size=1)(x)\n",
        "  conc = keras.layers.concatenate([avg_pl, max_pl])\n",
        "  conc = keras.layers.Conv1D(nb_filter, 5, padding='same')(conc)\n",
        "  conc = keras.layers.Activation('sigmoid')(conc)\n",
        "  sa = keras.layers.multiply([x, conc])\n",
        "  return sa\n",
        "\n",
        "def channel_attn_1D(x, nb_filter, reduction = 4):\n",
        "\n",
        "  avg_pl = keras.layers.AveragePooling1D(pool_size=1)(x)\n",
        "  avg_pl = keras.layers.Activation('relu')(avg_pl)\n",
        "\n",
        "  max_pl = keras.layers.MaxPooling1D(pool_size=1)(x)\n",
        "  max_pl = keras.layers.Activation('relu')(max_pl)\n",
        "\n",
        "  dense1 = keras.layers.Dense(nb_filter//reduction)\n",
        "  dense2 = keras.layers.Dense(nb_filter)\n",
        "\n",
        "  avg_pl_out = dense1(avg_pl)\n",
        "  avg_pl_out = dense2(avg_pl_out)\n",
        "\n",
        "  max_pl_out = dense1(max_pl)\n",
        "  max_pl_out = dense2(max_pl_out)\n",
        "\n",
        "  ca = keras.layers.add([avg_pl_out, max_pl_out])\n",
        "  ca = keras.layers.Activation('sigmoid')(ca)\n",
        "\n",
        "  return keras.layers.multiply([x, ca])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#2D models\n",
        "def lstm2D():\n",
        "  inputs = keras.Input((patch_size, patch_size, 2000))\n",
        "  x = keras.layers.TimeDistributed(keras.layers.LSTM(128, return_sequences=True))(inputs)\n",
        "  x = keras.layers.LeakyReLU()(x)\n",
        "  x = keras.layers.BatchNormalization()(x)\n",
        "  x = keras.layers.TimeDistributed(keras.layers.LSTM(64, return_sequences=True))(x)\n",
        "  x = keras.layers.LeakyReLU()(x)\n",
        "  x = keras.layers.BatchNormalization()(x)\n",
        "  x = keras.layers.TimeDistributed(keras.layers.LSTM(16, return_sequences=True))(x)\n",
        "  x = keras.layers.Dense(2, activation='relu')(x)\n",
        "  \n",
        "  model = keras.models.Model(inputs = inputs, outputs = x)\n",
        "  #Total params: 1,145,425\n",
        "  return model\n",
        "\n",
        "def conv2D(spatial_attention = False, channel_attention = False):\n",
        "  inputs = keras.Input((patch_size, patch_size, 2000))\n",
        "  \n",
        "  #x = channel_attn_2D(inputs, 2000, 62 )\n",
        "  x = keras.layers.Conv2D(32, 3, padding = 'same')(inputs)\n",
        "  x = keras.layers.Activation('relu')(x)\n",
        "  x = keras.layers.Conv2D(64, 3, padding = 'same')(x)\n",
        "  x = keras.layers.Activation('relu')(x)\n",
        "\n",
        "  if channel_attention:\n",
        "    x = channel_attn_2D(x, 64, 4 )\n",
        "\n",
        "  if spatial_attention:\n",
        "    x = spatial_attn_2D(x, 64) #Spatial attention\n",
        "\n",
        "\n",
        "  x = keras.layers.Conv2D(128, 3, padding = 'same')(x)\n",
        "  x = keras.layers.Activation('relu')(x)\n",
        "  x = keras.layers.Conv2D(64, 3, padding = 'same')(x)\n",
        "  x = keras.layers.Activation('relu')(x)\n",
        " \n",
        "  \n",
        "  x = keras.layers.Dense(2, activation = 'relu')(x)\n",
        "\n",
        "  return keras.models.Model(inputs = inputs, outputs = x)\n",
        "\n",
        "def spatial_attn_2D(x, nb_filter):\n",
        "  \n",
        "  avg_pl = keras.layers.AveragePooling2D(pool_size=patch_size)(x)\n",
        "  max_pl = keras.layers.MaxPooling2D(pool_size=patch_size)(x)\n",
        "  conc = keras.layers.concatenate([avg_pl, max_pl])\n",
        "  conc = keras.layers.Conv2D(nb_filter, 3, padding='same')(conc)\n",
        "  conc = keras.layers.Activation('sigmoid')(conc)\n",
        "  sa = keras.layers.multiply([x, conc])\n",
        "  return sa\n",
        "\n",
        "def channel_attn_2D(x, nb_filter, reduction = 4):\n",
        "\n",
        "  avg_pl = keras.layers.AveragePooling2D(pool_size=patch_size)(x)\n",
        "  avg_pl = keras.layers.Activation('relu')(avg_pl)\n",
        "\n",
        "  max_pl = keras.layers.MaxPooling2D(pool_size=patch_size)(x)\n",
        "  max_pl = keras.layers.Activation('relu')(max_pl)\n",
        "\n",
        "  dense1 = keras.layers.Dense(nb_filter//reduction)\n",
        "  dense2 = keras.layers.Dense(nb_filter)\n",
        "\n",
        "  avg_pl_out = dense1(avg_pl)\n",
        "  avg_pl_out = dense2(avg_pl_out)\n",
        "\n",
        "  max_pl_out = dense1(max_pl)\n",
        "  max_pl_out = dense2(max_pl_out)\n",
        "\n",
        "  ca = keras.layers.add([avg_pl_out, max_pl_out])\n",
        "  ca = keras.layers.Activation('sigmoid')(ca)\n",
        "\n",
        "  return keras.layers.multiply([x, ca])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIQKsU8ky8QR",
        "colab_type": "code",
        "outputId": "4a23a342-0da6-4d0b-a406-e3bfdd562c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Train a model\n",
        "model = conv1D()\n",
        "model_name = t_type + '_' + input_shape + '_' + 'conv1D' +'_'+ test\n",
        "\n",
        "lr = 0.0015\n",
        "decay = 0\n",
        "batch_size = 600\n",
        "num_epoch = 100\n",
        "\n",
        "model.compile(optimizer = keras.optimizers.adam(lr = lr, decay = decay),loss='mean_squared_error', metrics=['mean_absolute_error']) #fe_lr\n",
        "model.summary()\n",
        "\n",
        "filepath='/content/drive/My Drive/KCL internship/Task_1/Refik/fixed_lr_models/'+ model_name +'.hdf5'\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_mean_absolute_error', verbose=1, save_best_only=True, mode='auto')\n",
        "earlystopping = keras.callbacks.EarlyStopping(monitor='val_mean_absolute_error', mode='auto', patience=15, verbose=1)\n",
        "callbacks_list = [checkpoint, earlystopping]\n",
        "\n",
        "hist = model.fit(signal, T_val, validation_split = 0.3, epochs = num_epoch, batch_size = batch_size, callbacks=callbacks_list) \n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('model_loss')\n",
        "plt.ylabel('loss') \n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['loss','val_loss'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 1, 2000)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 1, 32)             192032    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1, 32)             0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 1, 64)             6208      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1, 64)             0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 1, 128)            24704     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 1, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 1, 64)             24640     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 1, 64)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1, 1)              65        \n",
            "=================================================================\n",
            "Total params: 247,649\n",
            "Trainable params: 247,649\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 61599 samples, validate on 26401 samples\n",
            "Epoch 1/100\n",
            "61599/61599 [==============================] - 7s 120us/step - loss: 0.0117 - mean_absolute_error: 0.0691 - val_loss: 0.0131 - val_mean_absolute_error: 0.0623\n",
            "\n",
            "Epoch 00001: val_mean_absolute_error improved from inf to 0.06227, saving model to /content/drive/My Drive/KCL internship/Task_1/Refik/fixed_lr_models/T1_pixel_conv1D_kl.hdf5\n",
            "Epoch 2/100\n",
            "61599/61599 [==============================] - 1s 19us/step - loss: 0.0064 - mean_absolute_error: 0.0393 - val_loss: 0.0107 - val_mean_absolute_error: 0.0428\n",
            "\n",
            "Epoch 00002: val_mean_absolute_error improved from 0.06227 to 0.04284, saving model to /content/drive/My Drive/KCL internship/Task_1/Refik/fixed_lr_models/T1_pixel_conv1D_kl.hdf5\n",
            "Epoch 3/100\n",
            "61599/61599 [==============================] - 1s 20us/step - loss: 0.0050 - mean_absolute_error: 0.0289 - val_loss: 0.0105 - val_mean_absolute_error: 0.0395\n",
            "\n",
            "Epoch 00003: val_mean_absolute_error improved from 0.04284 to 0.03947, saving model to /content/drive/My Drive/KCL internship/Task_1/Refik/fixed_lr_models/T1_pixel_conv1D_kl.hdf5\n",
            "Epoch 4/100\n",
            "61599/61599 [==============================] - 1s 20us/step - loss: 0.0046 - mean_absolute_error: 0.0263 - val_loss: 0.0104 - val_mean_absolute_error: 0.0450\n",
            "\n",
            "Epoch 00004: val_mean_absolute_error did not improve from 0.03947\n",
            "Epoch 5/100\n",
            "61599/61599 [==============================] - 1s 20us/step - loss: 0.0044 - mean_absolute_error: 0.0245 - val_loss: 0.0091 - val_mean_absolute_error: 0.0392\n",
            "\n",
            "Epoch 00005: val_mean_absolute_error improved from 0.03947 to 0.03916, saving model to /content/drive/My Drive/KCL internship/Task_1/Refik/fixed_lr_models/T1_pixel_conv1D_kl.hdf5\n",
            "Epoch 6/100\n",
            "61599/61599 [==============================] - 1s 20us/step - loss: 0.0041 - mean_absolute_error: 0.0240 - val_loss: 0.0090 - val_mean_absolute_error: 0.0392\n",
            "\n",
            "Epoch 00006: val_mean_absolute_error did not improve from 0.03916\n",
            "Epoch 7/100\n",
            "61599/61599 [==============================] - 1s 20us/step - loss: 0.0042 - mean_absolute_error: 0.0238 - val_loss: 0.0103 - val_mean_absolute_error: 0.0396\n",
            "\n",
            "Epoch 00007: val_mean_absolute_error did not improve from 0.03916\n",
            "Epoch 8/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0039 - mean_absolute_error: 0.0232 - val_loss: 0.0088 - val_mean_absolute_error: 0.0370\n",
            "\n",
            "Epoch 00008: val_mean_absolute_error improved from 0.03916 to 0.03705, saving model to /content/drive/My Drive/KCL internship/Task_1/Refik/fixed_lr_models/T1_pixel_conv1D_kl.hdf5\n",
            "Epoch 9/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0039 - mean_absolute_error: 0.0232 - val_loss: 0.0094 - val_mean_absolute_error: 0.0401\n",
            "\n",
            "Epoch 00009: val_mean_absolute_error did not improve from 0.03705\n",
            "Epoch 10/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0038 - mean_absolute_error: 0.0226 - val_loss: 0.0087 - val_mean_absolute_error: 0.0381\n",
            "\n",
            "Epoch 00010: val_mean_absolute_error did not improve from 0.03705\n",
            "Epoch 11/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0038 - mean_absolute_error: 0.0223 - val_loss: 0.0086 - val_mean_absolute_error: 0.0426\n",
            "\n",
            "Epoch 00011: val_mean_absolute_error did not improve from 0.03705\n",
            "Epoch 12/100\n",
            "61599/61599 [==============================] - 1s 20us/step - loss: 0.0038 - mean_absolute_error: 0.0226 - val_loss: 0.0082 - val_mean_absolute_error: 0.0390\n",
            "\n",
            "Epoch 00012: val_mean_absolute_error did not improve from 0.03705\n",
            "Epoch 13/100\n",
            "61599/61599 [==============================] - 1s 20us/step - loss: 0.0038 - mean_absolute_error: 0.0225 - val_loss: 0.0111 - val_mean_absolute_error: 0.0482\n",
            "\n",
            "Epoch 00013: val_mean_absolute_error did not improve from 0.03705\n",
            "Epoch 14/100\n",
            "61599/61599 [==============================] - 1s 20us/step - loss: 0.0037 - mean_absolute_error: 0.0222 - val_loss: 0.0087 - val_mean_absolute_error: 0.0402\n",
            "\n",
            "Epoch 00014: val_mean_absolute_error did not improve from 0.03705\n",
            "Epoch 15/100\n",
            "61599/61599 [==============================] - 1s 20us/step - loss: 0.0037 - mean_absolute_error: 0.0225 - val_loss: 0.0083 - val_mean_absolute_error: 0.0343\n",
            "\n",
            "Epoch 00015: val_mean_absolute_error improved from 0.03705 to 0.03431, saving model to /content/drive/My Drive/KCL internship/Task_1/Refik/fixed_lr_models/T1_pixel_conv1D_kl.hdf5\n",
            "Epoch 16/100\n",
            "61599/61599 [==============================] - 1s 20us/step - loss: 0.0037 - mean_absolute_error: 0.0225 - val_loss: 0.0083 - val_mean_absolute_error: 0.0358\n",
            "\n",
            "Epoch 00016: val_mean_absolute_error did not improve from 0.03431\n",
            "Epoch 17/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0036 - mean_absolute_error: 0.0218 - val_loss: 0.0080 - val_mean_absolute_error: 0.0378\n",
            "\n",
            "Epoch 00017: val_mean_absolute_error did not improve from 0.03431\n",
            "Epoch 18/100\n",
            "61599/61599 [==============================] - 1s 20us/step - loss: 0.0036 - mean_absolute_error: 0.0223 - val_loss: 0.0089 - val_mean_absolute_error: 0.0361\n",
            "\n",
            "Epoch 00018: val_mean_absolute_error did not improve from 0.03431\n",
            "Epoch 19/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0036 - mean_absolute_error: 0.0219 - val_loss: 0.0078 - val_mean_absolute_error: 0.0385\n",
            "\n",
            "Epoch 00019: val_mean_absolute_error did not improve from 0.03431\n",
            "Epoch 20/100\n",
            "61599/61599 [==============================] - 1s 20us/step - loss: 0.0035 - mean_absolute_error: 0.0216 - val_loss: 0.0080 - val_mean_absolute_error: 0.0364\n",
            "\n",
            "Epoch 00020: val_mean_absolute_error did not improve from 0.03431\n",
            "Epoch 21/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0035 - mean_absolute_error: 0.0218 - val_loss: 0.0081 - val_mean_absolute_error: 0.0365\n",
            "\n",
            "Epoch 00021: val_mean_absolute_error did not improve from 0.03431\n",
            "Epoch 22/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0035 - mean_absolute_error: 0.0219 - val_loss: 0.0078 - val_mean_absolute_error: 0.0416\n",
            "\n",
            "Epoch 00022: val_mean_absolute_error did not improve from 0.03431\n",
            "Epoch 23/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0035 - mean_absolute_error: 0.0220 - val_loss: 0.0093 - val_mean_absolute_error: 0.0384\n",
            "\n",
            "Epoch 00023: val_mean_absolute_error did not improve from 0.03431\n",
            "Epoch 24/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0034 - mean_absolute_error: 0.0215 - val_loss: 0.0074 - val_mean_absolute_error: 0.0327\n",
            "\n",
            "Epoch 00024: val_mean_absolute_error improved from 0.03431 to 0.03273, saving model to /content/drive/My Drive/KCL internship/Task_1/Refik/fixed_lr_models/T1_pixel_conv1D_kl.hdf5\n",
            "Epoch 25/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0033 - mean_absolute_error: 0.0206 - val_loss: 0.0077 - val_mean_absolute_error: 0.0325\n",
            "\n",
            "Epoch 00025: val_mean_absolute_error improved from 0.03273 to 0.03253, saving model to /content/drive/My Drive/KCL internship/Task_1/Refik/fixed_lr_models/T1_pixel_conv1D_kl.hdf5\n",
            "Epoch 26/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0033 - mean_absolute_error: 0.0204 - val_loss: 0.0072 - val_mean_absolute_error: 0.0328\n",
            "\n",
            "Epoch 00026: val_mean_absolute_error did not improve from 0.03253\n",
            "Epoch 27/100\n",
            "61599/61599 [==============================] - 1s 22us/step - loss: 0.0033 - mean_absolute_error: 0.0209 - val_loss: 0.0077 - val_mean_absolute_error: 0.0328\n",
            "\n",
            "Epoch 00027: val_mean_absolute_error did not improve from 0.03253\n",
            "Epoch 28/100\n",
            "61599/61599 [==============================] - 1s 22us/step - loss: 0.0033 - mean_absolute_error: 0.0204 - val_loss: 0.0072 - val_mean_absolute_error: 0.0338\n",
            "\n",
            "Epoch 00028: val_mean_absolute_error did not improve from 0.03253\n",
            "Epoch 29/100\n",
            "61599/61599 [==============================] - 1s 23us/step - loss: 0.0033 - mean_absolute_error: 0.0207 - val_loss: 0.0077 - val_mean_absolute_error: 0.0323\n",
            "\n",
            "Epoch 00029: val_mean_absolute_error improved from 0.03253 to 0.03226, saving model to /content/drive/My Drive/KCL internship/Task_1/Refik/fixed_lr_models/T1_pixel_conv1D_kl.hdf5\n",
            "Epoch 30/100\n",
            "61599/61599 [==============================] - 1s 23us/step - loss: 0.0032 - mean_absolute_error: 0.0202 - val_loss: 0.0071 - val_mean_absolute_error: 0.0362\n",
            "\n",
            "Epoch 00030: val_mean_absolute_error did not improve from 0.03226\n",
            "Epoch 31/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0032 - mean_absolute_error: 0.0199 - val_loss: 0.0075 - val_mean_absolute_error: 0.0341\n",
            "\n",
            "Epoch 00031: val_mean_absolute_error did not improve from 0.03226\n",
            "Epoch 32/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0033 - mean_absolute_error: 0.0205 - val_loss: 0.0076 - val_mean_absolute_error: 0.0386\n",
            "\n",
            "Epoch 00032: val_mean_absolute_error did not improve from 0.03226\n",
            "Epoch 33/100\n",
            "61599/61599 [==============================] - 1s 22us/step - loss: 0.0032 - mean_absolute_error: 0.0201 - val_loss: 0.0077 - val_mean_absolute_error: 0.0349\n",
            "\n",
            "Epoch 00033: val_mean_absolute_error did not improve from 0.03226\n",
            "Epoch 34/100\n",
            "61599/61599 [==============================] - 1s 22us/step - loss: 0.0033 - mean_absolute_error: 0.0207 - val_loss: 0.0077 - val_mean_absolute_error: 0.0373\n",
            "\n",
            "Epoch 00034: val_mean_absolute_error did not improve from 0.03226\n",
            "Epoch 35/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0033 - mean_absolute_error: 0.0202 - val_loss: 0.0075 - val_mean_absolute_error: 0.0300\n",
            "\n",
            "Epoch 00035: val_mean_absolute_error improved from 0.03226 to 0.03000, saving model to /content/drive/My Drive/KCL internship/Task_1/Refik/fixed_lr_models/T1_pixel_conv1D_kl.hdf5\n",
            "Epoch 36/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0032 - mean_absolute_error: 0.0199 - val_loss: 0.0071 - val_mean_absolute_error: 0.0306\n",
            "\n",
            "Epoch 00036: val_mean_absolute_error did not improve from 0.03000\n",
            "Epoch 37/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0031 - mean_absolute_error: 0.0196 - val_loss: 0.0072 - val_mean_absolute_error: 0.0308\n",
            "\n",
            "Epoch 00037: val_mean_absolute_error did not improve from 0.03000\n",
            "Epoch 38/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0031 - mean_absolute_error: 0.0194 - val_loss: 0.0075 - val_mean_absolute_error: 0.0408\n",
            "\n",
            "Epoch 00038: val_mean_absolute_error did not improve from 0.03000\n",
            "Epoch 39/100\n",
            "61599/61599 [==============================] - 1s 22us/step - loss: 0.0031 - mean_absolute_error: 0.0192 - val_loss: 0.0072 - val_mean_absolute_error: 0.0319\n",
            "\n",
            "Epoch 00039: val_mean_absolute_error did not improve from 0.03000\n",
            "Epoch 40/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0031 - mean_absolute_error: 0.0198 - val_loss: 0.0072 - val_mean_absolute_error: 0.0292\n",
            "\n",
            "Epoch 00040: val_mean_absolute_error improved from 0.03000 to 0.02920, saving model to /content/drive/My Drive/KCL internship/Task_1/Refik/fixed_lr_models/T1_pixel_conv1D_kl.hdf5\n",
            "Epoch 41/100\n",
            "61599/61599 [==============================] - 1s 22us/step - loss: 0.0032 - mean_absolute_error: 0.0194 - val_loss: 0.0072 - val_mean_absolute_error: 0.0369\n",
            "\n",
            "Epoch 00041: val_mean_absolute_error did not improve from 0.02920\n",
            "Epoch 42/100\n",
            "61599/61599 [==============================] - 1s 22us/step - loss: 0.0031 - mean_absolute_error: 0.0198 - val_loss: 0.0070 - val_mean_absolute_error: 0.0321\n",
            "\n",
            "Epoch 00042: val_mean_absolute_error did not improve from 0.02920\n",
            "Epoch 43/100\n",
            "61599/61599 [==============================] - 1s 22us/step - loss: 0.0031 - mean_absolute_error: 0.0193 - val_loss: 0.0075 - val_mean_absolute_error: 0.0308\n",
            "\n",
            "Epoch 00043: val_mean_absolute_error did not improve from 0.02920\n",
            "Epoch 44/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0032 - mean_absolute_error: 0.0195 - val_loss: 0.0073 - val_mean_absolute_error: 0.0298\n",
            "\n",
            "Epoch 00044: val_mean_absolute_error did not improve from 0.02920\n",
            "Epoch 45/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0031 - mean_absolute_error: 0.0193 - val_loss: 0.0071 - val_mean_absolute_error: 0.0348\n",
            "\n",
            "Epoch 00045: val_mean_absolute_error did not improve from 0.02920\n",
            "Epoch 46/100\n",
            "61599/61599 [==============================] - 1s 22us/step - loss: 0.0033 - mean_absolute_error: 0.0194 - val_loss: 0.0070 - val_mean_absolute_error: 0.0314\n",
            "\n",
            "Epoch 00046: val_mean_absolute_error did not improve from 0.02920\n",
            "Epoch 47/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0031 - mean_absolute_error: 0.0193 - val_loss: 0.0071 - val_mean_absolute_error: 0.0317\n",
            "\n",
            "Epoch 00047: val_mean_absolute_error did not improve from 0.02920\n",
            "Epoch 48/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0031 - mean_absolute_error: 0.0191 - val_loss: 0.0071 - val_mean_absolute_error: 0.0335\n",
            "\n",
            "Epoch 00048: val_mean_absolute_error did not improve from 0.02920\n",
            "Epoch 49/100\n",
            "61599/61599 [==============================] - 1s 22us/step - loss: 0.0031 - mean_absolute_error: 0.0194 - val_loss: 0.0071 - val_mean_absolute_error: 0.0322\n",
            "\n",
            "Epoch 00049: val_mean_absolute_error did not improve from 0.02920\n",
            "Epoch 50/100\n",
            "61599/61599 [==============================] - 1s 22us/step - loss: 0.0031 - mean_absolute_error: 0.0191 - val_loss: 0.0073 - val_mean_absolute_error: 0.0301\n",
            "\n",
            "Epoch 00050: val_mean_absolute_error did not improve from 0.02920\n",
            "Epoch 51/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0031 - mean_absolute_error: 0.0194 - val_loss: 0.0072 - val_mean_absolute_error: 0.0336\n",
            "\n",
            "Epoch 00051: val_mean_absolute_error did not improve from 0.02920\n",
            "Epoch 52/100\n",
            "61599/61599 [==============================] - 1s 22us/step - loss: 0.0030 - mean_absolute_error: 0.0189 - val_loss: 0.0069 - val_mean_absolute_error: 0.0308\n",
            "\n",
            "Epoch 00052: val_mean_absolute_error did not improve from 0.02920\n",
            "Epoch 53/100\n",
            "61599/61599 [==============================] - 1s 22us/step - loss: 0.0030 - mean_absolute_error: 0.0192 - val_loss: 0.0070 - val_mean_absolute_error: 0.0306\n",
            "\n",
            "Epoch 00053: val_mean_absolute_error did not improve from 0.02920\n",
            "Epoch 54/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0030 - mean_absolute_error: 0.0188 - val_loss: 0.0073 - val_mean_absolute_error: 0.0356\n",
            "\n",
            "Epoch 00054: val_mean_absolute_error did not improve from 0.02920\n",
            "Epoch 55/100\n",
            "61599/61599 [==============================] - 1s 21us/step - loss: 0.0030 - mean_absolute_error: 0.0189 - val_loss: 0.0070 - val_mean_absolute_error: 0.0306\n",
            "\n",
            "Epoch 00055: val_mean_absolute_error did not improve from 0.02920\n",
            "Epoch 00055: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a4k2gVOS5Gd",
        "colab_type": "code",
        "outputId": "be0c5591-b4b9-4546-d405-3d9b72914b66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "\"\"\"#Load the model\n",
        "from keras.models import load_model\n",
        "\n",
        "model_name = t_type + '_' + input_shape + '_' + 'conv2D_ca_2000' +'_'+ test\n",
        "filepath='/content/drive/My Drive/KCL internship/Task_1/Refik/fixed_lr_models/'+ model_name +'.hdf5'\n",
        "\n",
        "model = load_model(filepath)\n",
        "model.summary()\n",
        "\n",
        "#Pop the layers till attention score layer\n",
        "attn_sroce_model = keras.models.Model(input = model.inputs, outputs = model.get_layer(index=8).output) \n",
        "attn_sroce_model.summary()\n",
        "\n",
        "#Make prediction to get attention values\n",
        "attn_score = attn_sroce_model.predict(signal)\n",
        "print(\"attn shape:\", attn_score.shape)\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#Load the model\\nfrom keras.models import load_model\\n\\nmodel_name = t_type + \\'_\\' + input_shape + \\'_\\' + \\'conv2D_ca_2000\\' +\\'_\\'+ test\\nfilepath=\\'/content/drive/My Drive/KCL internship/Task_1/Refik/fixed_lr_models/\\'+ model_name +\\'.hdf5\\'\\n\\nmodel = load_model(filepath)\\nmodel.summary()\\n\\n#Pop the layers till attention score layer\\nattn_sroce_model = keras.models.Model(input = model.inputs, outputs = model.get_layer(index=8).output) \\nattn_sroce_model.summary()\\n\\n#Make prediction to get attention values\\nattn_score = attn_sroce_model.predict(signal)\\nprint(\"attn shape:\", attn_score.shape)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L-c0UcgR4Mt",
        "colab_type": "code",
        "outputId": "de5c721e-36a8-4c2d-b3bc-7541a7db8fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "\"\"\"#Visualize region and its channel attention analysis\n",
        "fig=plt.figure(figsize=(25, 7))\n",
        "fig.suptitle(\"Channel Attention Analysis\", fontsize=20)\n",
        "\n",
        "if input_shape == 'pixel':\n",
        "\n",
        "  attn_score_ab = attn_score[0:44000, :, :]\n",
        "  attn_score_ab_im = attn_score_ab.reshape(220, 200, 2000)\n",
        "\n",
        "  attn_score_tk = attn_score[44000: 88000, :, :]\n",
        "  \n",
        "  #print(\"attn_score_ab:\",attn_score_ab.shape)\n",
        "\n",
        "  pixel = 113\n",
        "  attn_score_ab_over_channel = attn_score_ab_over_channel.reshape(-1,1)\n",
        "  attn_score_ab_over_channel = attn_score_ab_im[pixel, pixel, :]\n",
        "  #print(\"attn_score_ab_im_pix0 shape:\",attn_score_ab_im_pix.shape)\n",
        "\n",
        "  T1_ab_pix_remarked = np.copy(T1_ab)\n",
        "  T1_ab_pix_remarked[pixel, pixel] = np.max(T1_ab)*3\n",
        "\n",
        "  plt.annotate(\"\", xy=(pixel-1, pixel-1), xytext=(pixel-15, pixel-15),\n",
        "            arrowprops=dict(width = 5.,\n",
        "                             headwidth = 15.,\n",
        "                            headlength = 5,\n",
        "                            shrink = 0.05,\n",
        "                            linewidth = 2, color = 'cyan'))\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(attn_score_tk_over_channel)\n",
        "\n",
        "  plt.title(\"[\"+str(pixel)+\",\"+str(pixel)+\"]\" + \" pixel's attention scores for each channel\")\n",
        "  plt.margins(0, 0)\n",
        "  plt.show()\n",
        "  \n",
        "    \n",
        "elif input_shape == 'patch':\n",
        "  \n",
        "  attn_score_ab_over_channel = attn_score[0:42749, :, :, :]\n",
        "  attn_score_ab_over_channel = attn_score_ab_over_channel.reshape(-1, attn_score_ab_over_channel.shape[3])\n",
        "\n",
        "  attn_score_tk_over_channel = attn_score[42749: 85498, :, :, :]\n",
        "  attn_score_tk_over_channel = attn_score_tk_over_channel.reshape(-1, attn_score_tk_over_channel.shape[3])\n",
        "  \n",
        "\n",
        "  patch = 22000\n",
        "  max_patch_for_a_row = (200-(patch_size - 1))\n",
        "  patch_row = patch//max_patch_for_a_row\n",
        "  patch_col = patch%(max_patch_for_a_row)\n",
        "  #print(\"patch_row:\"+ str(patch_row)+ \" patch_col:\"+ str(patch_col))\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(T1_ab, vmin=0, vmax=6000)\n",
        "  rectangle = plt.Rectangle((patch_col, patch_row), patch_size, patch_size, linewidth=3, edgecolor='r', facecolor='none')\n",
        "\n",
        "  plt.gca().add_patch(rectangle)\n",
        "  plt.colorbar()\n",
        "\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(attn_score_ab_over_channel[patch])\n",
        "\n",
        "  plt.title(str(patch)+\"th patch attention scores for each channel\")\n",
        "  plt.margins(0, 0)\n",
        "  \"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#Visualize region and its channel attention analysis\\nfig=plt.figure(figsize=(25, 7))\\nfig.suptitle(\"Channel Attention Analysis\", fontsize=20)\\n\\nif input_shape == \\'pixel\\':\\n\\n  attn_score_ab = attn_score[0:44000, :, :]\\n  attn_score_ab_im = attn_score_ab.reshape(220, 200, 2000)\\n\\n  attn_score_tk = attn_score[44000: 88000, :, :]\\n  \\n  #print(\"attn_score_ab:\",attn_score_ab.shape)\\n\\n  pixel = 113\\n  attn_score_ab_over_channel = attn_score_ab_over_channel.reshape(-1,1)\\n  attn_score_ab_over_channel = attn_score_ab_im[pixel, pixel, :]\\n  #print(\"attn_score_ab_im_pix0 shape:\",attn_score_ab_im_pix.shape)\\n\\n  T1_ab_pix_remarked = np.copy(T1_ab)\\n  T1_ab_pix_remarked[pixel, pixel] = np.max(T1_ab)*3\\n\\n  plt.annotate(\"\", xy=(pixel-1, pixel-1), xytext=(pixel-15, pixel-15),\\n            arrowprops=dict(width = 5.,\\n                             headwidth = 15.,\\n                            headlength = 5,\\n                            shrink = 0.05,\\n                            linewidth = 2, color = \\'cyan\\'))\\n\\n  plt.subplot(1,2,2)\\n  plt.plot(attn_score_tk_over_channel)\\n\\n  plt.title(\"[\"+str(pixel)+\",\"+str(pixel)+\"]\" + \" pixel\\'s attention scores for each channel\")\\n  plt.margins(0, 0)\\n  plt.show()\\n  \\n    \\nelif input_shape == \\'patch\\':\\n  \\n  attn_score_ab_over_channel = attn_score[0:42749, :, :, :]\\n  attn_score_ab_over_channel = attn_score_ab_over_channel.reshape(-1, attn_score_ab_over_channel.shape[3])\\n\\n  attn_score_tk_over_channel = attn_score[42749: 85498, :, :, :]\\n  attn_score_tk_over_channel = attn_score_tk_over_channel.reshape(-1, attn_score_tk_over_channel.shape[3])\\n  \\n\\n  patch = 22000\\n  max_patch_for_a_row = (200-(patch_size - 1))\\n  patch_row = patch//max_patch_for_a_row\\n  patch_col = patch%(max_patch_for_a_row)\\n  #print(\"patch_row:\"+ str(patch_row)+ \" patch_col:\"+ str(patch_col))\\n\\n  plt.subplot(1, 2, 1)\\n  plt.imshow(T1_ab, vmin=0, vmax=6000)\\n  rectangle = plt.Rectangle((patch_col, patch_row), patch_size, patch_size, linewidth=3, edgecolor=\\'r\\', facecolor=\\'none\\')\\n\\n  plt.gca().add_patch(rectangle)\\n  plt.colorbar()\\n\\n\\n  plt.subplot(1, 2, 2)\\n  plt.plot(attn_score_ab_over_channel[patch])\\n\\n  plt.title(str(patch)+\"th patch attention scores for each channel\")\\n  plt.margins(0, 0)\\n  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}